{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cf603c",
   "metadata": {},
   "source": [
    "# Обучения CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b05a545",
   "metadata": {},
   "source": [
    "## Используемые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a59248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import notebook\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e28cd",
   "metadata": {},
   "source": [
    "## Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dcdc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 21\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "SELFIE_PATH = 'путь до папки с тренировочным набором фотографий'\n",
    "ANIME_PATH = 'путь до папки с тренировочным набором аниме изображений'\n",
    "SELFIE_TEST_PATH = 'путь до папки с тестовым набором фотографий'\n",
    "ANIME_TEST_PATH = 'путь до папки с тестовым набором аниме изображений'\n",
    "\n",
    "# Общие константы:\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 25\n",
    "EPOCHS = 200\n",
    "\n",
    "# Константы для оптимизатора:\n",
    "LEARNING_RATE = 0.0003\n",
    "BETA_ONE = 0.5\n",
    "BETA_TWO = 0.999\n",
    "\n",
    "# Константы для шедулера:\n",
    "STEP = 101\n",
    "GAMMA = 0.8\n",
    "\n",
    "# Данные константы понадобятся, если Вы решите дообучать свою модель\n",
    "# с другими параметрами или датасетами:\n",
    "D_SELFIE = 'путь до папки с весами для фото-дискриминатора'\n",
    "D_ANIME = 'путь до папки с весами для аниме-дискриминатора'\n",
    "G_SELFIE_TO_ANIME = 'путь до папки с весами для генератора аниме из фотографий'\n",
    "G_ANIME_TO_SELFIE = 'путь до папки с весами для генератора фотографий из аниме'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d4f64",
   "metadata": {},
   "source": [
    "## Формирование датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58af780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfieToAnimeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, directory: str) -> Dataset:\n",
    "        \n",
    "        path_list = os.listdir(directory)\n",
    "        abspath = os.path.abspath(directory)\n",
    "        \n",
    "        self.directory = directory\n",
    "        self.image_list = [os.path.join(abspath, path) for path in path_list]\n",
    "        \n",
    "        # Аугментации для преобразования полученных от пользователя изображений:\n",
    "        self.transform = tt.Compose([\n",
    "            tt.Resize(IMAGE_SIZE),\n",
    "            tt.CenterCrop(IMAGE_SIZE),\n",
    "            tt.RandomHorizontalFlip(p=0.5),\n",
    "            tt.ToTensor(),\n",
    "            tt.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.image_list[index]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        \n",
    "        return self.transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67041c84",
   "metadata": {},
   "source": [
    "## Дискриминатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d66f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, features: int = 64) -> torch.Tensor:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, features, kernel_size=4, \n",
    "                      stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "            nn.Conv2d(features, features*2, kernel_size=4, \n",
    "                      stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(features*2, features*4, kernel_size=4, \n",
    "                      stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(features*4, features*8, kernel_size=4, \n",
    "                      stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(features*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        \n",
    "            nn.Conv2d(features*8, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.model(x)\n",
    "        x = F.avg_pool2d(x, x.size()[2:])\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d9a237",
   "metadata": {},
   "source": [
    "## Генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0fb51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features: int) -> torch.Tensor:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(features, features, kernel_size=3),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(features, features, kernel_size=3),\n",
    "            nn.InstanceNorm2d(features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abc05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_features: int = 3, \n",
    "                 features: int = 64) -> torch.Tensor:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            nn.ReflectionPad2d(in_features),\n",
    "            nn.Conv2d(in_features, features,\n",
    "                      kernel_size=7, stride=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(features, features*2,\n",
    "                      kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(features*2, features*4,\n",
    "                      kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ResidualBlock(features*4),\n",
    "            ResidualBlock(features*4),\n",
    "            ResidualBlock(features*4),\n",
    "            ResidualBlock(features*4),\n",
    "            ResidualBlock(features*4),\n",
    "            ResidualBlock(features*4),\n",
    "            ResidualBlock(features*4),\n",
    "            ResidualBlock(features*4),\n",
    "            ResidualBlock(features*4),\n",
    "        \n",
    "            nn.ConvTranspose2d(features*4, features*2, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(features*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(features*2, features, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ReflectionPad2d(in_features),\n",
    "            nn.Conv2d(features, in_features, kernel_size=7, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34c431",
   "metadata": {},
   "source": [
    "## CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db50e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Инициализация CUDA-ядер, при наличии:\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        self.G_selfie_to_anime = Generator().to(self.device)\n",
    "        self.G_anime_to_selfie = Generator().to(self.device)\n",
    "        \n",
    "        self.D_selfie = Discriminator().to(self.device)\n",
    "        self.D_anime = Discriminator().to(self.device)\n",
    "    \n",
    "    def load_weights(self, G_selfie_to_anime: str, G_anime_to_selfie: str,\n",
    "                     D_selfie: str, D_anime: str):\n",
    "        \"\"\"\n",
    "        Функция загружает необходимые веса для работы модели.\n",
    "\n",
    "        Параметры:\n",
    "        G_selfie_to_anime - путь до директории с весами\n",
    "                            для генератора \"из фотографии в аниме\";\n",
    "        G_anime_to_selfie - путь до директории с весами\n",
    "                            для генератора \"из аниме в фотографию\";\n",
    "        D_selfie - путь до директории с весами\n",
    "                   для дискриминатора для \"фотографий\";\n",
    "        D_anime - путь до директории с весами\n",
    "                  для дискриминатора для \"аниме\".\n",
    "        \"\"\"\n",
    "        self.G_selfie_to_anime.load_state_dict(torch.load(G_SELFIE_TO_ANIME))\n",
    "        self.G_anime_to_selfie.load_state_dict(torch.load(G_ANIME_TO_SELFIE))\n",
    "        self.D_selfie.load_state_dict(torch.load(D_SELFIE))\n",
    "        self.D_anime.load_state_dict(torch.load(D_ANIME))\n",
    "    \n",
    "    def real_mse_loss(self, D_out: torch.Tensor):\n",
    "        return torch.mean((D_out-1)**2)\n",
    "    \n",
    "    def fake_mse_loss(self, D_out: torch.Tensor):\n",
    "        return torch.mean(D_out**2)\n",
    "    \n",
    "    def cycle_consistency_loss(self, real_img: torch.Tensor, \n",
    "                               reconstructed_img: torch.Tensor, lambda_w: int | float):\n",
    "        return (torch.mean(torch.abs(real_img - reconstructed_img))) * lambda_w\n",
    "    \n",
    "    def train_generator(self, optimizer, selfie: torch.Tensor, anime: torch.Tensor):\n",
    "        \n",
    "        optimizer['generator'].zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            \n",
    "            # Обучаем генератор \"из аниме в фотографию\":\n",
    "            fake_selfie = self.G_anime_to_selfie(anime)\n",
    "            real_selfie = self.D_selfie(fake_selfie)\n",
    "            G_anime_to_selfie_loss = self.real_mse_loss(real_selfie)\n",
    "            reconstructed_anime = self.G_selfie_to_anime(fake_selfie)\n",
    "            reconstructed_anime_loss = self.cycle_consistency_loss(anime, reconstructed_anime, 10)\n",
    "            \n",
    "            # Обучаем генератор \"из фотографии в аниме\":\n",
    "            fake_anime = self.G_selfie_to_anime(selfie)\n",
    "            real_anime = self.D_anime(fake_anime)\n",
    "            G_selfie_to_anime_loss = self.real_mse_loss(real_anime)\n",
    "            reconstructed_selfie = self.G_anime_to_selfie(fake_anime)\n",
    "            reconstructed_selfie_loss = self.cycle_consistency_loss(selfie, reconstructed_selfie, 10)\n",
    "        \n",
    "            G_loss = (G_anime_to_selfie_loss + \n",
    "                      reconstructed_anime_loss +\n",
    "                      G_selfie_to_anime_loss +\n",
    "                      reconstructed_selfie_loss)\n",
    "        \n",
    "        G_loss.backward()\n",
    "        optimizer['generator'].step()\n",
    "        \n",
    "        return G_loss.item()\n",
    "    \n",
    "    def train_discriminator(self, optimizer: torch.optim.AdamW, \n",
    "                            selfie: torch.Tensor, anime: torch.Tensor,\n",
    "                            real_noise: torch.Tensor,\n",
    "                            fake_noise: torch.Tensor):\n",
    "        \n",
    "        optimizer['selfie_discriminator'].zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            \n",
    "            # Обучаем фото-дискриминатор с добавлением шума:\n",
    "            real_selfie = self.D_selfie(selfie)\n",
    "            real_selfie_loss = self.real_mse_loss(real_selfie - real_noise)\n",
    "            fake_selfie_images = self.G_anime_to_selfie(anime)\n",
    "            fake_selfie = self.D_selfie(fake_selfie_images)\n",
    "            fake_selfie_loss = self.fake_mse_loss(fake_selfie + fake_noise)\n",
    "\n",
    "            D_selfie_loss = real_selfie_loss + fake_selfie_loss\n",
    "        \n",
    "        D_selfie_loss.backward()\n",
    "        optimizer['selfie_discriminator'].step()\n",
    "        \n",
    "        optimizer['anime_discriminator'].zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            \n",
    "            # Обучаем аниме-дискриминатор с добавлением шума:\n",
    "            real_anime = self.D_anime(anime)\n",
    "            real_anime_loss = self.real_mse_loss(real_anime - real_noise)\n",
    "            fake_anime_image = self.G_selfie_to_anime(selfie)\n",
    "            fake_anime = self.D_anime(fake_anime_image)\n",
    "            fake_anime_loss = self.fake_mse_loss(fake_anime + fake_noise)\n",
    "\n",
    "            D_anime_loss = real_anime_loss + fake_anime_loss\n",
    "        \n",
    "        D_anime_loss.backward()\n",
    "        optimizer['anime_discriminator'].step()\n",
    "        \n",
    "        return D_selfie_loss.item(), D_anime_loss.item()\n",
    "    \n",
    "    def train(self, optimizer: torch.optim.AdamW, sheduler: StepLR, selfie_loader: DataLoader, \n",
    "              anime_loader: DataLoader, test_selfie_loader: DataLoader) -> tuple:\n",
    "        \n",
    "        losses = []   \n",
    "        \n",
    "        for epoch in notebook.tqdm(range(EPOCHS)):\n",
    "            for (selfie_image, anime_image) in notebook.tqdm(zip(selfie_loader, anime_loader)):\n",
    "                \n",
    "                # Переносим батчи на СUDA-ядра:\n",
    "                selfie = selfie_image.to(self.device)\n",
    "                anime = anime_image.to(self.device)\n",
    "                \n",
    "                # генерируем шум:\n",
    "                real_noise = 0.05 * torch.rand(selfie.size(0), 1, device=self.device)\n",
    "                fake_noise = 0.05 * torch.rand(selfie.size(0), 1, device=self.device)\n",
    "                \n",
    "                # обучаем CycleGAN:\n",
    "                G_loss = self.train_generator(optimizer, selfie, anime)\n",
    "                D_selfie_loss, D_anime_loss = self.train_discriminator(\n",
    "                    optimizer, selfie, anime, real_noise, fake_noise)\n",
    "                \n",
    "                losses.append((D_selfie_loss, D_anime_loss, G_loss))\n",
    "            \n",
    "            # Делаем шаг шедулером:\n",
    "            sheduler['generator'].step()\n",
    "            sheduler['selfie_discriminator'].step()\n",
    "            sheduler['anime_discriminator'].step()\n",
    "            \n",
    "            # При необходимости выводим Learning Rate:\n",
    "            print(sheduler['anime_discriminator'].get_last_lr())\n",
    "            \n",
    "            # Выводим общую информацию:\n",
    "            print('Epoch [{}/{}] | D_selfie_loss: {:.4f} | D_anime_loss: {:.4f} | G_loss: {:.4f}'.format(\n",
    "                epoch+1, EPOCHS, D_selfie_loss, D_anime_loss, G_loss))\n",
    "            \n",
    "            # Сохраняем веса каждого шага:\n",
    "            torch.save(self.G_selfie_to_anime.state_dict(), str(epoch + 1) + 'G_selfie_to_anime')\n",
    "            torch.save(self.G_anime_to_selfie.state_dict(), str(epoch + 1) + 'G_anime_to_selfie')\n",
    "            torch.save(self.D_selfie.state_dict(), str(epoch + 1) + 'D_selfie')\n",
    "            torch.save(self.D_anime.state_dict(), str(epoch + 1) + 'D_anime')\n",
    "            \n",
    "            # Визуализируем результат обучения для каждой эпохи:\n",
    "            samples = []\n",
    "            with torch.no_grad():\n",
    "                for i in range(2):\n",
    "                    fixed_selfie = next(iter(test_selfie_loader))[i].to(self.device)\n",
    "                    fake_anime = self.G_selfie_to_anime(fixed_selfie)\n",
    "                    samples.append(fixed_selfie)\n",
    "                    samples.append(fake_anime)\n",
    "            \n",
    "            plt.figure(figsize=(10, 10))\n",
    "            title = ['Selfie', 'Anime', 'Selfie', 'Anime']\n",
    "                \n",
    "            for i in range(4):\n",
    "                plt.subplot(1, 4, i+1)\n",
    "                plt.axis('off')\n",
    "                plt.title(title[i])\n",
    "                plt.imshow((samples[i] * 0.5 + 0.5).cpu().detach().permute(1, 2, 0))\n",
    "            plt.show();\n",
    "            \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cdcc0d",
   "metadata": {},
   "source": [
    "## Подготовка к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f9bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём датасеты:\n",
    "selfie_dataset = SelfieToAnimeDataset(SELFIE_PATH)\n",
    "anime_dataset = SelfieToAnimeDataset(ANIME_PATH)\n",
    "test_selfie_dataset = SelfieToAnimeDataset(SELFIE_TEST_PATH)\n",
    "test_anime_dataset = SelfieToAnimeDataset(ANIME_TEST_PATH)\n",
    "\n",
    "# Создаём объекты класса DataLoader:\n",
    "selfie_loader = DataLoader(selfie_dataset, BATCH_SIZE, shuffle=True)\n",
    "anime_loader = DataLoader(anime_dataset, BATCH_SIZE, shuffle=True)\n",
    "test_selfie_loader = DataLoader(test_selfie_dataset, BATCH_SIZE, shuffle=True)\n",
    "test_anime_loader = DataLoader(test_anime_dataset, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Инициализируем веса:\n",
    "model = CycleGAN()\n",
    "\n",
    "# Если планируем продолжить обучение нашей модели с определённого момента,\n",
    "# раскомментирем строчку ниже и инициализием веса модели:\n",
    "# --------------------------------------------------------------------------\n",
    "#model.load_weights(G_SELFIE_TO_ANIME, G_ANIME_TO_SELFIE, D_SELFIE, D_ANIME)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Объединим параметры генератора\n",
    "generator_params: list = (list(model.G_selfie_to_anime.parameters()) +\n",
    "                          list(model.G_anime_to_selfie.parameters()))\n",
    "\n",
    "# Зададим параметры для оптимизатора:\n",
    "optimizer = {\n",
    "    'generator': torch.optim.AdamW(\n",
    "        generator_params, LEARNING_RATE, [BETA_ONE, BETA_TWO]),\n",
    "    'selfie_discriminator': torch.optim.AdamW(\n",
    "        model.D_selfie.parameters(), LEARNING_RATE, [BETA_ONE, BETA_TWO]),\n",
    "    'anime_discriminator': torch.optim.AdamW(\n",
    "        model.D_anime.parameters(), LEARNING_RATE, [BETA_ONE, BETA_TWO])\n",
    "}\n",
    "\n",
    "# Зададим параметры для шедулера:\n",
    "sheduler = {\n",
    "    'generator': StepLR(optimizer['generator'], step_size=STEP, gamma=GAMMA),\n",
    "    'selfie_discriminator': StepLR(optimizer['selfie_discriminator'], step_size=STEP, gamma=GAMMA),\n",
    "    'anime_discriminator': StepLR(optimizer['anime_discriminator'], step_size=STEP, gamma=GAMMA)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13c412",
   "metadata": {},
   "source": [
    "## Обучение CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7142850",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = model.train(\n",
    "    optimizer,\n",
    "    sheduler,\n",
    "    selfie_loader,\n",
    "    anime_loader,\n",
    "    test_selfie_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
